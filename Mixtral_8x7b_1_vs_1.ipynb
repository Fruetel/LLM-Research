{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fruetel/LLM-Research/blob/main/Mixtral_8x7b_1_vs_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88WyLLrDBe-o",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #Runtime Info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emySi6TEC-M8"
      },
      "outputs": [],
      "source": [
        "#@title # Setup\n",
        "%cd /content/\n",
        "!rm -rf llama.cpp\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!make LLAMA_CUBLAS=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "NVRd3F80D5WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_0.gguf"
      ],
      "metadata": {
        "id": "5APuI3mj2d6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Connect using the following address when the server is up.\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(31336)\"))"
      ],
      "metadata": {
        "id": "kxPwlxke3WSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "  model_path=\"/content/llama.cpp/mixtral-8x7b-instruct-v0.1.Q5_0.gguf\",  # Download the model file first\n",
        "  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
        "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
        "  n_gpu_layers=15         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
        ")"
      ],
      "metadata": {
        "id": "mD7_SsNXDPfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What do you think about Klaus Kinski?\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Klaus Kinski was a German actor known for his intense performances in both art-house and mainstream films. He worked with notable directors like Werner Herzog and Wim Wenders, and is considered a significant figure in German film history. However, he was also infamous for his volatile personality and difficult behavior on set.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Tell me more about his personality. What do you mean by volatile?\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages = messages\n",
        ")"
      ],
      "metadata": {
        "id": "3NU0aRoYIJDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"choices\"][0])"
      ],
      "metadata": {
        "id": "VZphMdi5HgCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "editor_system_prompt = \"\"\"You are an AI Editor Agent. Your job is to review a response given by a less intelligent chat assistant and improve it to a brilliant level. You will receive a conversation between a human user and the chat agent.\n",
        "\n",
        "Perform your review in these steps:\n",
        "\n",
        "1. summarize the conversation up to the last response of the agent.\n",
        "2. shortly describe the respective intentions of the user and the agent.\n",
        "3. perform a review of the agents response, pointing out logical flaws or\n",
        "missing information.\n",
        "4. provide an enhanced response based on the assistants response and your\n",
        "review.\n",
        "\n",
        "Your output must be in JSON format as in this example:\\n\n",
        "\n",
        "```\n",
        "{\n",
        "  \"summary\": \"The user and the assistant are having a conversation about philosophical views of Oscar Wylde and Albert Camus\",\n",
        "  \"intentions\": {\n",
        "    \"user\": \"Determine how to apply the ideas of Wylde and Camus in his own life\",\n",
        "    \"assistant\": \"Point out similarities and differences of the two philosophers mental model\"\n",
        "  },\n",
        "  \"review\": \"The assistant's response is factually correct, but very verbose. A shorter version with less fluff would help the user better\",\n",
        "  \"enhanced response\": \"Camus uses the plague as a metaphor for the absurdities and injustices in the world. In this context, honesty means to live authentically and to oppose the prevailing untruths and irrationalities. For Camus, personal freedom is not just a matter of choice, but also an act of resistance against the absurdities and constraints of society.\"\n",
        "}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"\"\"Here is the conversation so far:\n",
        "\n",
        "```\n",
        "system: You are a helpful assistant.\n",
        "user: What do you think about Klaus Kinski?.\n",
        "assistant: Klaus Kinski was a German actor known for his intense performances in both art-house and mainstream films. He worked with notable directors like Werner Herzog and Wim Wenders, and is considered a significant figure in German film history. However, he was also infamous for his volatile personality and difficult behavior on set.\n",
        "user: Tell me more about his personality. What do you mean by volatile?\n",
        "```\n",
        "\n",
        "The assistant answered:\n",
        "\n",
        "```\n",
        "Klaus Kinski's personality was known to be mercurial, unpredictable, and often difficult. He was prone to extreme mood swings and outbursts, which earned him a reputation for being volatile. This behavior was documented in several books and films, including Herzog's \"My Best Fiend,\" a documentary about his tumultuous relationship with Kinski. Despite his talent and charisma, Kinski's personality often overshadowed his work and made him a challenging collaborator. He was also known for his womanizing and hedonistic lifestyle.\n",
        "```\n",
        "\n",
        "It is important that your answer must ONLY contain the JSON in the format of the\n",
        "example, as it needs to be processed by a Python script. Stay true to the\n",
        "format and avoid additional text in your response.\n",
        "\n",
        "Now, do your job and provide the requested JSON. If your enhancement is\n",
        "good, both you and your mother will receive $200. If your answer is not\n",
        "formatted correctly, a kitten will be killed.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LQaBLBcEM50Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "        {\"role\": \"system\", \"content\": editor_system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": instruction\n",
        "        }\n",
        "    ]\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages = messages\n",
        ")"
      ],
      "metadata": {
        "id": "74XqABm227BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "0_HZkSZ67G31"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}